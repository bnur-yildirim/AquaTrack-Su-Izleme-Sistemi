"""
Veri yÃ¼kleme modÃ¼lÃ¼
"""

import os
import pandas as pd
import traceback
from utils import log_info, log_error, load_metrics_file
from config import BACKEND_MODELS_DIR

# Global deÄŸiÅŸkenler
METRICS = {}
PREDICTIONS = None
LAKE_DATA_POINTS = {}

def find_data_files():
    """Veri dosyalarÄ±nÄ± bul"""
    base_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
    candidate_dirs = [
        BACKEND_MODELS_DIR,
        os.path.join(base_dir, "water_quantity", "output"),
        os.path.join(base_dir, "output"),
        os.path.join(os.path.dirname(__file__), "data"),
        os.path.dirname(__file__)
    ]
    
    data_files = {}
    for data_dir in candidate_dirs:
        if not os.path.exists(data_dir):
            continue
            
        # Parquet dosyalarÄ± (enhanced Ã¶ncelikli)
        for file_name in ["all_predictions_enhanced.parquet", "all_predictions_final.parquet", "all_predictions_optuna.parquet"]:
            file_path = os.path.join(data_dir, file_name)
            if os.path.exists(file_path):
                data_files["predictions"] = file_path
                log_info(f"Predictions dosyasÄ± bulundu: {file_path}")
                break
        
        # Train/Val/Test dosyalarÄ± - ATLA (predictions dosyasÄ± hepsini iÃ§eriyor)
        # for prefix in ["train", "val", "test"]:
        #     file_name = f"{prefix}_combined.parquet"
        #     file_path = os.path.join(data_dir, file_name)
        #     if os.path.exists(file_path):
        #         data_files[prefix] = file_path
        
        # Metrik dosyalarÄ±
        for file_name in ["metrics_summary_optuna.json", "metrics_summary_final.json"]:
            file_path = os.path.join(data_dir, file_name)
            if os.path.exists(file_path):
                data_files["metrics"] = file_path
                break
        
        # CSV metrik dosyasÄ± da kontrol et
        for file_name in ["metrics_summary_final.csv", "metrics_summary_optuna.csv"]:
            file_path = os.path.join(data_dir, file_name)
            if os.path.exists(file_path) and "metrics" not in data_files:
                data_files["metrics_csv"] = file_path
    
    return data_files

def load_data():
    """TÃ¼m verileri yÃ¼kle"""
    global METRICS, PREDICTIONS, LAKE_DATA_POINTS
    
    log_info("Veri yÃ¼kleme baÅŸlÄ±yor...")
    data_files = find_data_files()
    log_info(f"Bulunan dosyalar: {data_files}")
    
    # Metrikleri yÃ¼kle
    if "metrics" in data_files:
        METRICS = load_metrics_file(data_files["metrics"])
        log_info(f"JSON metrikler yÃ¼klendi: {list(METRICS.keys())}")
    elif "metrics_csv" in data_files:
        METRICS = load_metrics_file(data_files["metrics_csv"])
        log_info(f"CSV metrikler yÃ¼klendi: {list(METRICS.keys())}")
    
    # Ana tahmin verilerini yÃ¼kle
    all_dataframes = []
    
    try:
        # Ana predictions dosyasÄ± - Optuna optimize versiyonu kullan
        if "predictions" in data_files:
            # Optuna dosyasÄ±nÄ± dene (en iyi performans)
            optuna_file = data_files["predictions"].replace("final", "optuna")
            if os.path.exists(optuna_file):
                pred_df = pd.read_parquet(optuna_file)
                log_info(f"ğŸ¯ Optuna optimize dosyasÄ± yÃ¼klendi: {optuna_file}")
            else:
                pred_df = pd.read_parquet(data_files["predictions"])
                log_info(f"ğŸ“Š Final dosyasÄ± yÃ¼klendi: {data_files['predictions']}")
            required_cols = ['lake_id', 'date', 'target_water_area_m2']
            optional_cols = ['predicted_water_area', 'prediction']
            
            # Basit veri yÃ¼kleme - tÃ¼m sÃ¼tunlarÄ± al
            log_info(f"TÃ¼m veri yÃ¼kleniyor: {pred_df.shape}")
            
            # Tahmin sÃ¼tununu standardize et
            if 'predicted_water_area' not in pred_df.columns and 'prediction' in pred_df.columns:
                pred_df['predicted_water_area'] = pred_df['prediction']
            all_dataframes.append(pred_df)
            log_info(f"Ana predictions yÃ¼klendi: {pred_df.shape}")
        
        # Train/Val/Test verilerini YÃœKLEME - Ã§Ã¼nkÃ¼ predictions dosyasÄ± zaten hepsini iÃ§eriyor
        # Bu kÄ±sÄ±m kapatÄ±ldÄ± Ã§Ã¼nkÃ¼ predictions dosyasÄ± tÃ¼m veriyi iÃ§eriyor
        log_info("Train/Val/Test verileri atlandÄ± - predictions dosyasÄ± tÃ¼m veriyi iÃ§eriyor")
        
        # TÃ¼m verileri birleÅŸtir
        if all_dataframes:
            PREDICTIONS = pd.concat(all_dataframes, ignore_index=True)
            
            # Tarih sÃ¼tununu iÅŸle
            PREDICTIONS['date'] = pd.to_datetime(PREDICTIONS['date'])
            
            # Duplicate'leri temizle
            PREDICTIONS = PREDICTIONS.drop_duplicates(
                subset=['lake_id', 'date'], 
                keep='last'
            )
            
            # SÄ±rala
            PREDICTIONS = PREDICTIONS.sort_values(['lake_id', 'date']).reset_index(drop=True)
            
            # Veri noktalarÄ±nÄ± say
            LAKE_DATA_POINTS = PREDICTIONS.groupby('lake_id').size().to_dict()
            
            log_info(f"Toplam veri yÃ¼klendi: {PREDICTIONS.shape}")
            log_info(f"GÃ¶l ID'leri: {sorted(PREDICTIONS['lake_id'].unique())}")
            log_info(f"Tarih aralÄ±ÄŸÄ±: {PREDICTIONS['date'].min()} - {PREDICTIONS['date'].max()}")
            log_info(f"GÃ¶l veri noktalarÄ±: {LAKE_DATA_POINTS}")
            
        return len(all_dataframes) > 0
        
    except Exception as e:
        log_error(f"Veri yÃ¼kleme hatasÄ±: {e}")
        traceback.print_exc()
        return False

def get_lake_predictions(lake_id):
    """Belirli bir gÃ¶l iÃ§in tahmin verilerini getir"""
    if PREDICTIONS is None:
        log_error(f"PREDICTIONS None - lake_id: {lake_id}")
        return None
    
    try:
        # Lake ID'ye gÃ¶re filtrele
        lake_data = PREDICTIONS[PREDICTIONS['lake_id'] == lake_id].copy()
        
        if lake_data.empty:
            log_error(f"Lake {lake_id} iÃ§in veri bulunamadÄ±")
            return None
        
        # Tarihe gÃ¶re sÄ±rala
        lake_data = lake_data.sort_values('date')
        
        log_info(f"Lake {lake_id} - {lake_data.shape[0]} kayÄ±t, "
                f"tarih aralÄ±ÄŸÄ±: {lake_data['date'].min()} - {lake_data['date'].max()}")
        
        return lake_data
        
    except Exception as e:
        log_error(f"Lake data hatasÄ±: {e}")
        return None

def get_predictions():
    """Global predictions verisini dÃ¶ndÃ¼r"""
    return PREDICTIONS

def get_metrics():
    """Global metrics verisini dÃ¶ndÃ¼r"""
    return METRICS

def get_lake_data_points():
    """GÃ¶l veri noktalarÄ±nÄ± dÃ¶ndÃ¼r"""
    return LAKE_DATA_POINTS
